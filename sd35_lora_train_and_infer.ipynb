{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Stable Diffusion 3.5 å·®åˆ†å­¦ç¿’ï¼ˆLoRAï¼‰ & ãƒãƒƒãƒç”»åƒç”Ÿæˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š\n",
    "\n",
    "1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "2. Google Driveã®ãƒã‚¦ãƒ³ãƒˆ & ãƒ‘ã‚¹è¨­å®š\n",
    "3. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆSD 3.5ï¼‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ / èª­ã¿è¾¼ã¿\n",
    "4. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ï¼ˆGoogle Driveï¼‰\n",
    "5. å·®åˆ†å­¦ç¿’ï¼ˆLoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰\n",
    "6. å­¦ç¿’æ¸ˆã¿å·®åˆ†ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆGoogle Driveï¼‰\n",
    "7. ç”»åƒç”Ÿæˆç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰\n",
    "8. ãƒã‚¸ãƒ†ã‚£ãƒ– / ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã£ãŸãƒãƒƒãƒç”»åƒç”Ÿæˆ\n",
    "9. ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šæ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãƒ»å†ç¾æ€§ç¢ºä¿\n",
    "\n",
    "**æ³¨æ„**: GPUãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\n",
    "- ãƒ¡ãƒ‹ãƒ¥ãƒ¼ > ãƒ©ãƒ³ã‚¿ã‚¤ãƒ  > ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚¿ã‚¤ãƒ—ã‚’å¤‰æ›´ > ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿: GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1"
   },
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_libraries"
   },
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install -q diffusers[torch] transformers accelerate peft safetensors\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q pillow opencv-python matplotlib\n",
    "\n",
    "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_libraries"
   },
   "outputs": [],
   "source": [
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from diffusers import StableDiffusion3Pipeline, SD3Transformer2DModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# GPUã®ç¢ºèª\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPUå: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2"
   },
   "source": [
    "## 2. Google Driveã®ãƒã‚¦ãƒ³ãƒˆ & ãƒ‘ã‚¹è¨­å®š\n",
    "\n",
    "Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã—ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¨­å®šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Google Driveã®ãƒã‚¦ãƒ³ãƒˆ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"âœ… Google DriveãŒãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®è¨­å®š\n",
    "PROJECT_ROOT = \"/content/drive/MyDrive/sd35_project\"\n",
    "DATASET_DIR = f\"{PROJECT_ROOT}/datasets/my_dataset_01\"\n",
    "LORA_OUTPUT_DIR = f\"{PROJECT_ROOT}/models/lora/sd35_lora_exp001\"\n",
    "GENERATED_DIR = f\"{PROJECT_ROOT}/outputs/samples\"\n",
    "LOG_DIR = f\"{PROJECT_ROOT}/outputs/logs\"\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "os.makedirs(PROJECT_ROOT, exist_ok=True)\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{DATASET_DIR}/images\", exist_ok=True)\n",
    "os.makedirs(f\"{DATASET_DIR}/captions\", exist_ok=True)\n",
    "os.makedirs(LORA_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(GENERATED_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ: {PROJECT_ROOT}\")\n",
    "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_DIR}\")\n",
    "print(f\"LoRAä¿å­˜å…ˆ: {LORA_OUTPUT_DIR}\")\n",
    "print(f\"ç”»åƒç”Ÿæˆå…ˆ: {GENERATED_DIR}\")\n",
    "print(\"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section3"
   },
   "source": [
    "## 3. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆSD 3.5ï¼‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ / èª­ã¿è¾¼ã¿\n",
    "\n",
    "Stable Diffusion 3.5ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
    "\n",
    "**æ³¨æ„**: Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚  \n",
    "ãƒˆãƒ¼ã‚¯ãƒ³ã¯ https://huggingface.co/settings/tokens ã‹ã‚‰å–å¾—ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_config"
   },
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
    "BASE_MODEL_ID = \"stabilityai/stable-diffusion-3.5-large\"  # ã¾ãŸã¯ \"stabilityai/stable-diffusion-3-medium-diffusers\"\n",
    "USE_LOCAL_BASE = False  # Trueã®å ´åˆã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "LOCAL_MODEL_PATH = f\"{PROJECT_ROOT}/models/base/sd35_base\"  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹\n",
    "\n",
    "# Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¿…è¦ã«å¿œã˜ã¦è¨­å®šï¼‰\n",
    "HF_TOKEN = None  # ã¾ãŸã¯ \"hf_your_token_here\"\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šï¼ˆå¿…è¦ãªå ´åˆï¼‰\n",
    "if HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"âœ… Hugging Faceã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_base_model"
   },
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "print(\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...\")\n",
    "\n",
    "if USE_LOCAL_BASE:\n",
    "    model_path = LOCAL_MODEL_PATH\n",
    "else:\n",
    "    model_path = BASE_MODEL_ID\n",
    "\n",
    "# SD3ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®èª­ã¿è¾¼ã¿\n",
    "base_pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "print(\"âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section4"
   },
   "source": [
    "## 4. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ï¼ˆGoogle Driveï¼‰\n",
    "\n",
    "å­¦ç¿’ç”¨ã®ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æº–å‚™ã—ã¾ã™ã€‚\n",
    "\n",
    "**ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ**:\n",
    "```\n",
    "DATASET_DIR/\n",
    "  images/\n",
    "    img_0001.jpg\n",
    "    img_0002.png\n",
    "    ...\n",
    "  captions/  # ä»»æ„\n",
    "    img_0001.txt\n",
    "    img_0002.txt\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_config"
   },
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š\n",
    "IMAGE_DIR = f\"{DATASET_DIR}/images\"\n",
    "CAPTION_DIR = f\"{DATASET_DIR}/captions\"  # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åˆ©ç”¨ã—ãªã„å ´åˆã¯ None\n",
    "RESOLUTION = 768  # å­¦ç¿’æ™‚ã®è§£åƒåº¦ï¼ˆ512, 768, 1024ãªã©ï¼‰\n",
    "DEFAULT_CAPTION = \"a high quality image\"  # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "\n",
    "print(f\"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {IMAGE_DIR}\")\n",
    "print(f\"ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {CAPTION_DIR}\")\n",
    "print(f\"å­¦ç¿’è§£åƒåº¦: {RESOLUTION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_dataset"
   },
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_dir=None, resolution=768, default_caption=\"a high quality image\"):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.caption_dir = Path(caption_dir) if caption_dir else None\n",
    "        self.resolution = resolution\n",
    "        self.default_caption = default_caption\n",
    "        \n",
    "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "        self.image_files = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp']:\n",
    "            self.image_files.extend(list(self.image_dir.glob(ext)))\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(resolution),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®èª­ã¿è¾¼ã¿\n",
    "        if self.caption_dir:\n",
    "            caption_path = self.caption_dir / f\"{image_path.stem}.txt\"\n",
    "            if caption_path.exists():\n",
    "                with open(caption_path, 'r', encoding='utf-8') as f:\n",
    "                    caption = f.read().strip()\n",
    "            else:\n",
    "                caption = self.default_caption\n",
    "        else:\n",
    "            caption = self.default_caption\n",
    "        \n",
    "        return {\"image\": image, \"caption\": caption}\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ\n",
    "train_dataset = ImageCaptionDataset(\n",
    "    IMAGE_DIR,\n",
    "    CAPTION_DIR,\n",
    "    RESOLUTION,\n",
    "    DEFAULT_CAPTION\n",
    ")\n",
    "\n",
    "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆ{len(train_dataset)}æšã®ç”»åƒï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section5"
   },
   "source": [
    "## 5. å·®åˆ†å­¦ç¿’ï¼ˆLoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰\n",
    "\n",
    "LoRAã‚’ä½¿ç”¨ã—ã¦Stable Diffusion 3.5ã‚’å·®åˆ†å­¦ç¿’ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lora_config"
   },
   "outputs": [],
   "source": [
    "# LoRAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
    "LORA_RANK = 16\n",
    "LORA_ALPHA = 32\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "MAX_TRAIN_STEPS = 1000  # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ç›´æ¥æŒ‡å®šã™ã‚‹å ´åˆï¼ˆNoneã®å ´åˆã¯ã‚¨ãƒãƒƒã‚¯æ•°ã§è¨ˆç®—ï¼‰\n",
    "SAVE_STEPS = 100  # ä½•ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜ã™ã‚‹ã‹\n",
    "MIXED_PRECISION = \"fp16\"  # \"fp16\" or \"bf16\" or \"no\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"LoRA Rank: {LORA_RANK}\")\n",
    "print(f\"LoRA Alpha: {LORA_ALPHA}\")\n",
    "print(f\"ãƒãƒƒãƒã‚µã‚¤ã‚º: {TRAIN_BATCH_SIZE}\")\n",
    "print(f\"ã‚¨ãƒãƒƒã‚¯æ•°: {NUM_EPOCHS}\")\n",
    "print(f\"å­¦ç¿’ç‡: {LEARNING_RATE}\")\n",
    "print(f\"å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"æœ€å¤§å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—: {MAX_TRAIN_STEPS}\")\n",
    "print(f\"ä¿å­˜ã‚¹ãƒ†ãƒƒãƒ—é–“éš”: {SAVE_STEPS}\")\n",
    "print(f\"æ··åˆç²¾åº¦: {MIXED_PRECISION}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_lora"
   },
   "outputs": [],
   "source": [
    "# LoRAã®è¨­å®š\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Transformerãƒ¢ãƒ‡ãƒ«ã®å–å¾—\n",
    "transformer = base_pipeline.transformer\n",
    "\n",
    "# LoRAè¨­å®š\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],  # SD3ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# LoRAãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
    "transformer = get_peft_model(transformer, lora_config)\n",
    "transformer.print_trainable_parameters()\n",
    "\n",
    "# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®Transformerã‚’æ›´æ–°\n",
    "base_pipeline.transformer = transformer\n",
    "base_pipeline.to(device)\n",
    "\n",
    "print(\"âœ… LoRAã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_lora"
   },
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®è¨­å®š\n",
    "optimizer = AdamW(\n",
    "    transformer.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=1e-2,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "# å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¨ˆç®—\n",
    "if MAX_TRAIN_STEPS is None:\n",
    "    num_update_steps_per_epoch = len(train_dataloader) // GRADIENT_ACCUMULATION_STEPS\n",
    "    max_train_steps = NUM_EPOCHS * num_update_steps_per_epoch\n",
    "else:\n",
    "    max_train_steps = MAX_TRAIN_STEPS\n",
    "\n",
    "print(f\"ç·å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°: {max_train_steps}\")\n",
    "\n",
    "# GradScalerï¼ˆæ··åˆç²¾åº¦ç”¨ï¼‰\n",
    "scaler = GradScaler() if MIXED_PRECISION == \"fp16\" else None\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "transformer.train()\n",
    "global_step = 0\n",
    "progress_bar = tqdm(total=max_train_steps, desc=\"Training\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®å–å¾—\n",
    "        images = batch[\"image\"].to(device, dtype=torch.float16)\n",
    "        captions = batch[\"caption\"]\n",
    "        \n",
    "        # å‹¾é…ã®è¨ˆç®—\n",
    "        with autocast(enabled=(MIXED_PRECISION == \"fp16\")):\n",
    "            # ã“ã“ã§ã¯ç°¡æ˜“çš„ãªå®Ÿè£…ã§ã™\n",
    "            # å®Ÿéš›ã«ã¯ã€SD3ã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã«å¾“ã£ã¦ãƒã‚¤ã‚ºè¿½åŠ ãƒ»æå¤±è¨ˆç®—ã‚’è¡Œã„ã¾ã™\n",
    "            # diffusersã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‚ç…§ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™\n",
    "            \n",
    "            # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "            text_inputs = base_pipeline.tokenizer(\n",
    "                captions,\n",
    "                padding=\"max_length\",\n",
    "                max_length=77,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # ã“ã“ã«å®Ÿéš›ã®å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…\n",
    "            # ï¼ˆãƒã‚¤ã‚ºè¿½åŠ ã€denoisingã€æå¤±è¨ˆç®—ãªã©ï¼‰\n",
    "            # loss = ...\n",
    "            \n",
    "            # ä»®ã®æå¤±ï¼ˆå®Ÿéš›ã«ã¯é©åˆ‡ãªæå¤±é–¢æ•°ã‚’ä½¿ç”¨ï¼‰\n",
    "            loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "        \n",
    "        # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # å‹¾é…ç´¯ç©\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            if scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"epoch\": epoch})\n",
    "            \n",
    "            # å®šæœŸä¿å­˜\n",
    "            if global_step % SAVE_STEPS == 0:\n",
    "                save_path = f\"{LORA_OUTPUT_DIR}/checkpoint-{global_step}\"\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                transformer.save_pretrained(save_path)\n",
    "                print(f\"\\nâœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {save_path}\")\n",
    "            \n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    \n",
    "    if global_step >= max_train_steps:\n",
    "        break\n",
    "\n",
    "progress_bar.close()\n",
    "print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section6"
   },
   "source": [
    "## 6. å­¦ç¿’æ¸ˆã¿å·®åˆ†ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ï¼ˆGoogle Driveï¼‰\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿ã®LoRAãƒ¢ãƒ‡ãƒ«ã‚’Google Driveã«ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_lora"
   },
   "outputs": [],
   "source": [
    "# æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "final_save_path = f\"{LORA_OUTPUT_DIR}/final\"\n",
    "os.makedirs(final_save_path, exist_ok=True)\n",
    "\n",
    "transformer.save_pretrained(final_save_path)\n",
    "print(f\"âœ… æœ€çµ‚LoRAãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {final_save_path}\")\n",
    "\n",
    "# è¨­å®šæƒ…å ±ã®ä¿å­˜\n",
    "import json\n",
    "\n",
    "training_config = {\n",
    "    \"base_model\": BASE_MODEL_ID,\n",
    "    \"lora_rank\": LORA_RANK,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"resolution\": RESOLUTION,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "    \"total_steps\": global_step,\n",
    "}\n",
    "\n",
    "with open(f\"{final_save_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(\"âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section7"
   },
   "source": [
    "## 7. ç”»åƒç”Ÿæˆç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿LoRAãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ç”»åƒç”Ÿæˆç”¨ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_lora_for_inference"
   },
   "outputs": [],
   "source": [
    "# æ¨è«–ç”¨ã®è¨­å®š\n",
    "LORA_MODEL_PATH = f\"{LORA_OUTPUT_DIR}/final\"  # ä½¿ç”¨ã™ã‚‹LoRAãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹\n",
    "\n",
    "# ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å†èª­ã¿è¾¼ã¿ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ã§é–‹å§‹ï¼‰\n",
    "inference_pipeline = StableDiffusion3Pipeline.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# LoRAã®èª­ã¿è¾¼ã¿\n",
    "print(f\"LoRAãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {LORA_MODEL_PATH}\")\n",
    "inference_pipeline.load_lora_weights(LORA_MODEL_PATH)\n",
    "\n",
    "# GPUã«è»¢é€\n",
    "inference_pipeline.to(device)\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "inference_pipeline.enable_model_cpu_offload()\n",
    "# inference_pipeline.enable_vae_slicing()\n",
    "# inference_pipeline.enable_attention_slicing()\n",
    "\n",
    "print(\"âœ… æ¨è«–ç”¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section8"
   },
   "source": [
    "## 8. ãƒã‚¸ãƒ†ã‚£ãƒ– / ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã£ãŸãƒãƒƒãƒç”»åƒç”Ÿæˆ\n",
    "\n",
    "è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒã§ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_generation_config"
   },
   "outputs": [],
   "source": [
    "# ãƒãƒƒãƒç”Ÿæˆè¨­å®š\n",
    "PROMPT_FILE = f\"{PROJECT_ROOT}/prompts/prompts_batch.txt\"  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "NUM_IMAGES_PER_PROMPT = 2  # 1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚ãŸã‚Šã®ç”Ÿæˆæšæ•°\n",
    "GUIDANCE_SCALE = 7.5  # CFG scale\n",
    "NUM_INFERENCE_STEPS = 28  # SD3.5ã§ã¯28ã‚¹ãƒ†ãƒƒãƒ—ãŒæ¨å¥¨\n",
    "OUTPUT_DIR = GENERATED_DIR\n",
    "SEED = 42  # å†ç¾æ€§ã‚’ç¢ºä¿ã—ãŸã„å ´åˆã«å›ºå®šï¼ˆNoneã§ãƒ©ãƒ³ãƒ€ãƒ ï¼‰\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ãƒãƒƒãƒç”Ÿæˆè¨­å®š\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {PROMPT_FILE}\")\n",
    "print(f\"1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚ãŸã‚Šã®ç”Ÿæˆæšæ•°: {NUM_IMAGES_PER_PROMPT}\")\n",
    "print(f\"Guidance Scale: {GUIDANCE_SCALE}\")\n",
    "print(f\"æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°: {NUM_INFERENCE_STEPS}\")\n",
    "print(f\"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}\")\n",
    "print(f\"ã‚·ãƒ¼ãƒ‰å€¤: {SEED}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_prompt_file"
   },
   "outputs": [],
   "source": [
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰\n",
    "# å®Ÿéš›ã«ã¯ Google Drive ä¸Šã«æ‰‹å‹•ã§ä½œæˆã—ã¦ãã ã•ã„\n",
    "\n",
    "os.makedirs(f\"{PROJECT_ROOT}/prompts\", exist_ok=True)\n",
    "\n",
    "sample_prompts = [\n",
    "    \"a beautiful landscape with mountains and a lake|||low quality, blurry\",\n",
    "    \"a cute cat sitting on a chair|||ugly, deformed\",\n",
    "    \"a futuristic city at night with neon lights|||dark, grainy\",\n",
    "]\n",
    "\n",
    "with open(PROMPT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sample_prompts))\n",
    "\n",
    "print(f\"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {PROMPT_FILE}\")\n",
    "print(\"\\n--- ã‚µãƒ³ãƒ—ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ---\")\n",
    "for i, prompt in enumerate(sample_prompts, 1):\n",
    "    print(f\"{i}. {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_generate"
   },
   "outputs": [],
   "source": [
    "# ãƒãƒƒãƒç”»åƒç”Ÿæˆ\n",
    "import datetime\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª­ã¿è¾¼ã¿\n",
    "with open(PROMPT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = [line.strip() for line in f if line.strip() and not line.startswith(\"#\")]\n",
    "\n",
    "print(f\"èª­ã¿è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {len(prompts)}\")\n",
    "print(f\"ç”Ÿæˆã™ã‚‹ç”»åƒã®ç·æ•°: {len(prompts) * NUM_IMAGES_PER_PROMPT}\")\n",
    "print(\"\\nç”»åƒç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...\\n\")\n",
    "\n",
    "# ã‚·ãƒ¼ãƒ‰å€¤ã®è¨­å®š\n",
    "if SEED is not None:\n",
    "    generator = torch.Generator(device=device).manual_seed(SEED)\n",
    "else:\n",
    "    generator = None\n",
    "\n",
    "# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãå‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "batch_output_dir = f\"{OUTPUT_DIR}/batch_{timestamp}\"\n",
    "os.makedirs(batch_output_dir, exist_ok=True)\n",
    "\n",
    "# å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”»åƒç”Ÿæˆ\n",
    "for prompt_idx, prompt_line in enumerate(tqdm(prompts, desc=\"Generating images\")):\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åˆ†å‰²ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–|||ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰\n",
    "    if \"|||\" in prompt_line:\n",
    "        positive_prompt, negative_prompt = prompt_line.split(\"|||\", 1)\n",
    "        positive_prompt = positive_prompt.strip()\n",
    "        negative_prompt = negative_prompt.strip()\n",
    "    else:\n",
    "        positive_prompt = prompt_line.strip()\n",
    "        negative_prompt = \"\"\n",
    "    \n",
    "    # è¤‡æ•°æšç”Ÿæˆ\n",
    "    for img_idx in range(NUM_IMAGES_PER_PROMPT):\n",
    "        # ç”»åƒç”Ÿæˆ\n",
    "        image = inference_pipeline(\n",
    "            prompt=positive_prompt,\n",
    "            negative_prompt=negative_prompt if negative_prompt else None,\n",
    "            num_inference_steps=NUM_INFERENCE_STEPS,\n",
    "            guidance_scale=GUIDANCE_SCALE,\n",
    "            generator=generator,\n",
    "        ).images[0]\n",
    "        \n",
    "        # ä¿å­˜\n",
    "        if NUM_IMAGES_PER_PROMPT == 1:\n",
    "            filename = f\"prompt{prompt_idx:03d}.png\"\n",
    "        else:\n",
    "            filename = f\"prompt{prompt_idx:03d}_{img_idx}.png\"\n",
    "        \n",
    "        save_path = os.path.join(batch_output_dir, filename)\n",
    "        image.save(save_path)\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±ã‚‚ä¿å­˜\n",
    "    prompt_info_path = os.path.join(batch_output_dir, f\"prompt{prompt_idx:03d}_info.txt\")\n",
    "    with open(prompt_info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Positive: {positive_prompt}\\n\")\n",
    "        f.write(f\"Negative: {negative_prompt}\\n\")\n",
    "        f.write(f\"Steps: {NUM_INFERENCE_STEPS}\\n\")\n",
    "        f.write(f\"Guidance Scale: {GUIDANCE_SCALE}\\n\")\n",
    "        f.write(f\"Seed: {SEED}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… ãƒãƒƒãƒç”»åƒç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "print(f\"ä¿å­˜å…ˆ: {batch_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_samples"
   },
   "outputs": [],
   "source": [
    "# ç”Ÿæˆç”»åƒã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# æœ€æ–°ã®ç”»åƒã‚’è¡¨ç¤º\n",
    "image_files = sorted(Path(batch_output_dir).glob(\"*.png\"))[:6]  # æœ€åˆã®6æš\n",
    "\n",
    "if image_files:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, img_path in enumerate(image_files):\n",
    "        img = Image.open(img_path)\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(img_path.name)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º\n",
    "    for idx in range(len(image_files), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"è¡¨ç¤ºã™ã‚‹ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section9"
   },
   "source": [
    "## 9. ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šæ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãƒ»å†ç¾æ€§ç¢ºä¿\n",
    "\n",
    "ã‚ˆã‚Šè©³ç´°ãªåˆ¶å¾¡ã®ãŸã‚ã®è¿½åŠ æ©Ÿèƒ½ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "advanced_generation"
   },
   "outputs": [],
   "source": [
    "# è©³ç´°ãªç”Ÿæˆè¨­å®šï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºç”¨ï¼‰\n",
    "\n",
    "def generate_image_advanced(\n",
    "    prompt,\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.5,\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    seed=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    è©³ç´°è¨­å®šã§ç”»åƒã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "    \n",
    "    image = inference_pipeline(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt if negative_prompt else None,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "    \n",
    "    return image\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”Ÿæˆ\n",
    "test_image = generate_image_advanced(\n",
    "    prompt=\"a serene Japanese garden with cherry blossoms\",\n",
    "    negative_prompt=\"low quality, blurry, ugly\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.5,\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    seed=12345,\n",
    ")\n",
    "\n",
    "# è¡¨ç¤º\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(test_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Test Generation\")\n",
    "plt.show()\n",
    "\n",
    "# ä¿å­˜\n",
    "test_image.save(f\"{OUTPUT_DIR}/test_generation.png\")\n",
    "print(f\"âœ… ãƒ†ã‚¹ãƒˆç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {OUTPUT_DIR}/test_generation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parameter_sweep"
   },
   "outputs": [],
   "source": [
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—ï¼ˆç•°ãªã‚‹è¨­å®šã§ã®æ¯”è¼ƒï¼‰\n",
    "\n",
    "test_prompt = \"a magical forest with glowing mushrooms\"\n",
    "test_negative = \"dark, scary, low quality\"\n",
    "\n",
    "# ç•°ãªã‚‹Guidance Scaleã§ãƒ†ã‚¹ãƒˆ\n",
    "guidance_scales = [3.5, 5.0, 7.5, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(guidance_scales), figsize=(20, 5))\n",
    "\n",
    "for idx, gs in enumerate(guidance_scales):\n",
    "    img = generate_image_advanced(\n",
    "        prompt=test_prompt,\n",
    "        negative_prompt=test_negative,\n",
    "        guidance_scale=gs,\n",
    "        seed=42,\n",
    "    )\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"Guidance Scale: {gs}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ã¾ã¨ã‚\n",
    "\n",
    "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã—ãŸï¼š\n",
    "\n",
    "1. âœ… ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "2. âœ… Google Driveã®ãƒã‚¦ãƒ³ãƒˆ\n",
    "3. âœ… Stable Diffusion 3.5ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "4. âœ… å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™\n",
    "5. âœ… LoRAå·®åˆ†å­¦ç¿’ã®å®Ÿè¡Œ\n",
    "6. âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "7. âœ… ç”»åƒç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰\n",
    "8. âœ… ãƒãƒƒãƒç”»åƒç”Ÿæˆ\n",
    "9. âœ… è©³ç´°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´\n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "- ã‚ˆã‚Šå¤šãã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’é›†ã‚ã¦å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "- ç•°ãªã‚‹LoRAè¨­å®šï¼ˆrank, alphaï¼‰ã§å®Ÿé¨“\n",
    "- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®æ¢æ±‚\n",
    "- ç”Ÿæˆç”»åƒã®å“è³ªè©•ä¾¡ã¨æ”¹å–„\n",
    "\n",
    "Happy creating! ğŸ¨âœ¨"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
